




















\subsubsection{Méthodes du second ordre}

\indexfrr{apprentissage}{second ordre}%
\indexfrr{méthode}{second ordre} 
\indexfrr{ordre}{méthode du second ordre}
\label{rn_optim_second_ordre}

L'algorithme~\ref{rn_apprentissage_global} fournit le canevas des méthodes d'optimisation du second ordre. La mise à jour des coefficients (étape~\ref{algo_global1_step_maj}) est différente car elle prend en compte les dernières valeurs des coefficients ainsi que les derniers gradients calculés. Ce passé va être utilisé pour estimer une direction de recherche pour le minimum différente de celle du gradient, cette direction est appelée gradient conjugué (voir \citeindex{Moré1977}). \indexfrr{gradient}{conjugué}

Ces techniques sont basées sur une approximation du second degré de la fonction à minimiser. On note $M$ le nombre de coefficients du réseau de neurones (biais compris). Soit $h: \ensemblereel^{M} \dans \ensemblereel $ la fonction d'erreur associée au réseau de neurones~:

    $$
    h \pa {W} = \summyone{i} e \pa{Y_i,f \pa{ W,X_i} }
    $$

Au voisinage de $W_{0}$, un développement limité donne~:
    $$
    h \pa {W}     =   h\pa {W_0}  + \frac{\partial h\left( W_{0}\right)  }{\partial W}\left( W-W_{0}\right) +\left(
    W-W_{0}\right) ^{\prime}\frac{\partial^{2}h\left(  W_{0}\right)  }{\partial W^{2}}\left( W-W_{0}\right) +o\left\|
    W-W_{0}\right\|  ^{2}
    $$

Par conséquent, sur un voisinage de $W_{0}$, la fonction $h\left( W\right)$ admet un minimum local si
$\frac{\partial^{2}h\left( W_{0}\right) }{\partial W^{2}}$ est définie positive strictement\footnote{
    \para{Rappel :} $\dfrac{\partial^{2}h\left(  W_{0}\right)  }{\partial W^{2}%
    }$ est définie positive strictement $\Longleftrightarrow\forall Z\in\R^{N},\; Z\neq0\Longrightarrow
    Z^{\prime}\dfrac{\partial ^{2}h\left( W_{0}\right)  }{\partial W^{2}}Z>0$
    }. Une matrice symétrique définie strictement positive est inversible, et le minimum est atteint pour la valeur :
    
    
    \begin{eqnarray}
    W_{\min}= W_0 + \frac{1}{2}\left[  \dfrac{\partial^{2}h\left(  W_{0}\right) }
    		{\partial W^{2}}\right]  ^{-1}\left[  \frac{\partial h\left(  W_{0}\right)
    }{\partial W}\right] \label{rn_hessien}
    \end{eqnarray}

Néanmoins, pour un réseau de neurones, le calcul de la dérivée seconde est coûteux, son inversion également. C'est pourquoi les dernières valeurs des coefficients et du gradient sont utilisées afin d'approcher cette dérivée seconde ou directement son inverse. Deux méthodes d'approximation sont présentées~:

    \begin{enumerate}
    \item l'algorithme BFGS (Broyden-Fletcher-Goldfarb-Shano), \citeindex{Broyden1967}, \citeindex{Fletcher1993}
                                                                \indexfr{BFGS}
    \item l'algoritmhe DFP  (Davidon-Fletcher-Powell), \citeindex{Davidon1959}, \citeindex{Fletcher1963}
                                                                \indexfr{DFP}
    \end{enumerate}

La figure~\ref{figure_gradient_conjugue} est couramment employée pour illustrer l'intérêt des méthodes de gradient conjugué. \indexfrr{gradient}{conjugué} Le problème consiste à trouver le minimum d'une fonction quadratique, par exemple, $G\pa{x,y} = 3x^2 + y^2$. Tandis que le gradient est orthogonal aux lignes de niveaux de la fonction $G$, le gradient conjugué se dirige plus sûrement vers le minimum global.

		\begin{figure}[ht]
    \[
    \begin{tabular}{|c|} \hline
		\filefig{../rn/fig_rn_02}
    \\ \hline
    \end{tabular}
    \]
    \caption{Gradient et gradient conjugué sur une ligne de niveau de la fonction $G\pa{x,y} = 3x^2 + y^2$, 
    					le gradient est
              orthogonal aux lignes de niveaux de la fonction $G$, mais cette direction est rarement la bonne à moins
              que le point $\pa{x,y}$ se situe sur un des axes des ellipses, le gradient conjugué agrège les derniers
              déplacements et propose une direction de recherche plus plausible pour le minimum de la fonction.}
    \label{figure_gradient_conjugue}
		\end{figure}




Ces méthodes proposent une estimation de la dérivée seconde (ou de son inverse) utilisée en (\ref{rn_hessien}). Dans les méthodes du premier ordre, une itération permet de calculer les poids $W_{t+1}$ à partir des poids $W_t$ et du gradient $G_t$. Si ce gradient est petit, on peut supposer que $G_{t+1}$ est presque égal au produit de la dérivée seconde par $G_t$. Cette relation est mise à profit pour construire une estimation de la dérivée seconde. Cette matrice notée $B_t$ dans l'algorithme~\ref{rn_algo_bfgs} est d'abord supposée égale à l'identité puis actualisée à chaque itération en tenant de l'information apportée par chaque déplacement. 



		\begin{xalgorithm} {algorithme BFGS}
		\label{rn_algo_bfgs}%
		\indexfr{BFGS}
		
		Le nombre de paramètres de la fonction $f$ est $M$.
		
		\begin{xalgostep}{initialisation}
		    Le premier jeu de coefficients $W_0$ du réseau de neurones est choisi aléatoirement. \newline%
		    $
		    \begin{array}{lcl}
		    t   &\longleftarrow&    0 \\
		    E_0 &\longleftarrow&    \summy{i=1}{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}} \\
		    B_0 &\longleftarrow&    I_M \\
		    i   &\longleftarrow&    0
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{calcul du gradient}\label{algo_global_bfgs_step_back}
		    $
		    \begin{array}{lcl}
		    g_t &\longleftarrow& \partialfrac{E_t}{W} \pa {W_t}= \summy{i=1}{N} e'\pa {Y_{i} - f \pa{W_t,X_{i}}} \\
		    c_t &\longleftarrow& B_t g_t
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour des coefficients}\label{algo_global_bfgs_step_maj}
		    $
		    \begin{array}{lcl}
		    \epsilon^*  &\longleftarrow&    \underset{\epsilon}{\arg \inf} \; \summy{i=1}{N}
		    		 e\pa {Y_i - f \pa{W_t - \epsilon c_t,X_i}}  \\
		    W_{t+1}     &\longleftarrow&    W_t - \epsilon^* c_t \\
		    E_{t+1}     &\longleftarrow&    \summy{i=1}{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
		    t           &\longleftarrow&    t+1
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour de la matrice $B_t$}
		    \begin{xif}{   $t - i \supegal M$ ou %
		            $g'_{t-1} B_{t-1} g_{t-1} \infegal 0$ ou %
		            $g'_{t-1} B_{t-1} \pa {g_t - g_{t-1}} \infegal 0$}
		        $
		        \begin{array}{lcl}
		        B_{t}       &\longleftarrow&    I_M \\
		        i           &\longleftarrow&    t
		        \end{array}
		        $
		    \xelse
		        $
		        \begin{array}{lcl}
		        s_t         &\longleftarrow&    W_t - W_{t-1} \\
		        d_t         &\longleftarrow&    g_t - g_{t-1} \\
		        B_{t}       &\longleftarrow&    B_{t-1} +   \pa{1 + \dfrac{ d'_t B_{t-1} d_t}{d'_t s_t}}
		                                                            \dfrac{s_t s'_t} {s'_t d_t}
		                                                - \dfrac{s_t d'_t B_{t-1} +  B_{t-1} d_t s'_t } { d'_t s_t }
		        \end{array}
		        $
		    \end{xif}
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
		    si $\frac{E_t}{E_{t-1}} \approx 1 $ alors l'apprentissage a convergé sinon retour à
		    l'étape~\ref{algo_global_bfgs_step_back}
		\end{xalgostep}
		\end{xalgorithm}




Lorsque la matrice $B_t$ est égale à l'identité, le gradient conjugué est égal au gradient. Au fur et à mesure des itérations, cette matrice toujours symétrique évolue en améliorant la convergence de l'optimisation. Néanmoins, la matrice $B_t$ doit être "nettoyée" (égale à l'identité) fréquemment afin d'éviter qu'elle n'agrège un passé trop lointain. Elle est aussi nettoyée lorsque le gradient conjugué semble trop s'éloigner du véritable gradient et devient plus proche d'une direction perpendiculaire.

La convergence de cet algorithme dans le cas des réseaux de neurones est plus rapide qu'un algorithme du premier ordre, une preuve en est donnée dans \citeindex{Driancourt1996}.

En pratique, la recherche de $\epsilon^*$ est réduite car le calcul de l'erreur est souvent coûteux, il peut être effectué sur un grand nombre d'exemples. C'est pourquoi on remplace l'étape~\ref{algo_global_bfgs_step_maj} par celle-ci les étapes~\ref{algo_global_bfgs_p_step_back} et~\ref{algo_global_bfgs_p_step_back_2}~:



		\begin{xalgorithm} {algorithme BFGS'}
		\label{rn_algo_bfgs_prime}%
		\indexfr{BFGS'} Le nombre de paramètre de la fonction $f$ est $M$.
		
		\begin{xalgostep}{initialisation}
		    voir algorithme~\ref{rn_algo_bfgs}
		\end{xalgostep}
		
		\begin{xalgostep}{calcul du gradient}\label{algo_global_bfgs_p_step_back}
		    voir algorithme~\ref{rn_algo_bfgs}
		\end{xalgostep}
		
		\begin{xalgostep}{recherche de $\epsilon^*$}
		    $\epsilon^*  \longleftarrow    \epsilon_0$ \newline
		    \begin{xdowhile}{$E_{t+1} \supegal E_t$ et $\epsilon^* \gg 0$}
		        $
		        \begin{array}{lcl}
		        \epsilon^*  &\longleftarrow&   \frac{\epsilon^*}{2} \\
		        W_{t+1}     &\longleftarrow&    W_t - \epsilon^* c_t \\
		        E_{t+1}     &\longleftarrow&    \summy{i=1}{N} e\pa {Y_i - f \pa{W_{t+1},X_i}}
		        \end{array}
		        $
		    \end{xdowhile}\newline
		    \begin{xif}{$\epsilon_* \approx 0$ et $B_t \neq I_M$}
		        $
		        \begin{array}{lcl}
		        B_{t}       &\longleftarrow&    I_M \\
		        i           &\longleftarrow&    t
		        \end{array}
		        $ \newline
		        retour à l'étape~\ref{algo_global_bfgs_p_step_back}
		    \end{xif}
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour des coefficients}\label{algo_global_bfgs_p_step_back_2}
		    $
		    \begin{array}{lcl}
		    W_{t+1}     &\longleftarrow&    W_t - \epsilon^* c_t \\
		    E_{t+1}     &\longleftarrow&    \summy{i=1}{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
		    t           &\longleftarrow&    t+1
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour de la matrice $B_t$}
		    voir algorithme~\ref{rn_algo_bfgs}
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
		    voir algorithme~\ref{rn_algo_bfgs}
		\end{xalgostep}
		\end{xalgorithm}
		
		
L'algorithme DFP est aussi un algorithme de gradient conjugué qui propose une approximation différente de l'inverse de la dérivée seconde.
		
		
		\begin{xalgorithm} {algorithme DFP}
		\label{rn_algo_dfp}%
		\indexfr{DFP} Le nombre de paramètre de la fonction $f$ est $M$.
		
		\begin{xalgostep}{initialisation}
		    Le premier jeu de coefficients $W_0$ du réseau de neurones est choisi aléatoirement. \newline%
		    $
		    \begin{array}{lcl}
		    t   &\longleftarrow&    0 \\
		    E_0 &\longleftarrow&    \summy{i=1}{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}} \\
		    B_0 &\longleftarrow&    I_M \\
		    i   &\longleftarrow&    0
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{calcul du gradient}\label{algo_global_dfp_step_back}
		    $
		    \begin{array}{lcl}
		    g_t &\longleftarrow& \partialfrac{E_t}{W} \pa {W_t}= \summy{i=1}{N} e'\pa {Y_{i} - f \pa{W_t,X_{i}}} \\
		    c_t &\longleftarrow& B_t g_t
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour des coefficients}
		    $
		    \begin{array}{lcl}
		    \epsilon^*  &\longleftarrow&    \underset{\epsilon}{\arg \inf} \;
		    							 \summy{i=1}{N} e\pa {Y_i - f \pa{W_t - \epsilon c_t,X_i}}  \\
		    W_{t+1}     &\longleftarrow&    W_t - \epsilon^* c_t \\
		    E_{t+1}     &\longleftarrow&    \summy{i=1}{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
		    t           &\longleftarrow&    t+1
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{mise à jour de la matrice $B_t$}\label{algo_global_dfp_step_same}
		    \begin{xif}{   $t - i \supegal M$ ou %
		            $g'_{t-1} B_{t-1} g_{t-1} \infegal 0$ ou %
		            $g'_{t-1} B_{t-1} \pa {g_t - g_{t-1}} \infegal 0$}
		        $
		        \begin{array}{lcl}
		        B_{t}       &\longleftarrow&    I_M \\
		        i           &\longleftarrow&    t
		        \end{array}
		        $
		    \xelse
		        $
		        \begin{array}{lcl}
		        d_t         &\longleftarrow&    W_t - W_{t-1} \\
		        s_t         &\longleftarrow&    g_t - g_{t-1} \\
		        B_{t}       &\longleftarrow&    B_{t-1} +     \dfrac{d_t d'_t} {d'_t s_t}
		                                                    - \dfrac{B_{t-1} s_t s'_t B_{t-1} } { s'_t B_{t-1} s_t }
		        \end{array}
		        $
		    \end{xif}
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
		    si $\frac{E_t}{E_{t-1}} \approx 1 $ alors l'apprentissage a convergé sinon retour à    
		    l'étape~\ref{algo_global_dfp_step_back}
		\end{xalgostep}
		\end{xalgorithm}



Seule l'étape~\ref{algo_global_dfp_step_same} de mise à jour de $B_t$ diffère dans les algorithmes~\ref{rn_algo_bfgs} et~\ref{rn_algo_dfp}. Comme l'algorithme BFGS (\ref{rn_algo_bfgs}), on peut construire une version DFP' inspirée de l'algorithme~\ref{rn_algo_bfgs_prime}. \indexfr{DFP'} 











\subsection{Apprentissage avec gradient stochastique}

\indexfrr{apprentissage}{stochastique} 
\indexfr{stochastique}%

Compte tenu des courbes d'erreurs très "accidentées" (figure~\ref{figure_courbe_accident}) dessinées par les réseaux de neurones, il existe une multitude de minima locaux. De ce fait, l'apprentissage global converge rarement vers le minimum global de la fonction d'erreur lorsqu'on applique les algorithmes basés sur le gradient global. L'apprentissage avec gradient stochastique est une solution permettant de mieux explorer ces courbes d'erreurs. De plus, les méthodes de gradient conjugué nécessite le stockage d'une matrice trop grande parfois pour des fonctions ayant quelques milliers de paramètres. C'est pourquoi l'apprentissage avec gradient stochastique est souvent préféré à l'apprentissage global pour de grands réseaux de neurones alors que les méthodes du second ordre trop coûteuses en calcul sont cantonnées à de petits réseaux. En contrepartie, la convergence est plus lente. La démonstration de cette convergence nécessite l'utilisation de quasi-martingales et est une convergence presque sûre \citeindex{Bottou1991}.

		\begin{figure}[ht]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=3cm, width=6cm] 
    {\filext{../dessin2/errminloc}}\end{array}$}$$
    \caption{Exemple de minima locaux.}
    \label{figure_courbe_accident}
		\end{figure}


		\begin{xalgorithm} {apprentissage stochastique}
		\label{rn_algorithme_apprentissage_2}%
		\indexfr{stochastique}
		\indexfrr{optimisation}{stochastique}
		
		\begin{xalgostep}{initialisation}
		    Le premier jeu de coefficients $W_0$ du réseau de neurones est choisi aléatoirement. \newline%
		    $
		    \begin{array}{lcl}
		    t       &\longleftarrow&    0 \\
		    E_0 &\longleftarrow&    \summy{i=1}{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}}
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{récurrence}\label{algo_global_sto_step_back}
		    $W_{t,0} \longleftarrow    W_0$ \newline
		    \begin{xfor}{t'}{0}{N-1}
		        $
		        \begin{array}{lcl}
		        i           &\longleftarrow&    \text{ nombre aléatoire dans } \ensemble{1}{N} \\
		        g           &\longleftarrow&    \partialfrac{E}{W} \pa {W_{t,t'}}=  e'\pa {Y_{i} - f
		                                    \pa{W_{t,t'},X_{i}}} \\
		        W_{t,t'+1}  &\longleftarrow&    W_{t,t'} - \epsilon_t g
		        \end{array}
		        $
		    \end{xfor} \medskip \newline
		    $
		    \begin{array}{lcl}
		        W_{t+1}     &\longleftarrow& W_{t,N} \\
		        E_{t+1}     &\longleftarrow& \summy{i=1}{N} e\pa {Y_{i} - f \pa{W_{t+1},X_{i}}} \\
		        t           &\longleftarrow& t+1
		    \end{array}
		    $
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
		    si $\frac{E_t}{E_{t-1}} \approx 1 $ alors l'apprentissage a convergé sinon retour à
		    l'étape~\ref{algo_global_sto_step_back}.
		\end{xalgostep}
		\end{xalgorithm}
		

En pratique, il est utile de converser le meilleur jeu de coefficients : $W^* = \underset{u \supegal 0}{\arg \min} \; E_{u}$ car la suite $\pa {E_u}_{u \supegal 0}$ n'est pas une suite décroissante. 













%--------------------------------------------------------------------------------------------------------------------
\section{Classification} \label{classification}
%--------------------------------------------------------------------------------------------------------------------


\subsection{Vraisemblance d'un échantillon de variable suivant une loi multinomiale}


Soit $\pa{Y_i}_{1 \infegal i \infegal N}$ un échantillon de variables aléatoires i.i.d. \indexfr{i.i.d.} suivant la loi multinomiale $\loimultinomiale { \vecteurno{p_1}{p_C}}$. On définit~:
\indexfrr{loi}{multinomiale}%

    $$
    \forall k \in \intervalle{1}{C}, \; d_k = \frac{1}{N} \summy{i=1}{N} \indicatrice{Y_i = k}
    $$

La vraisemblance de l'échantillon est~:

    \begin{eqnarray}
    L\pa{\vecteurno{Y_1}{Y_N}, \vecteurno{p_1}{p_C}} &=& \prody{i=1}{n} p_{Y_i} \nonumber\\
    \ln L\pa{\vecteurno{Y_1}{Y_N}, \vecteurno{p_1}{p_C}} &=& \summy{i=1}{n} \ln p_{Y_i}  \nonumber\\
    \ln L\pa{\vecteurno{Y_1}{Y_N}, \vecteurno{p_1}{p_C}} &=& \summy{k=1}{C} \cro{ \pa{\ln p_k}
                                                                    \summy{i=1}{N}  \indicatrice{Y_i = k}}  \nonumber\\
    \ln L\pa{\vecteurno{Y_1}{Y_N}, \vecteurno{p_1}{p_C}} &=& N \summy{k=1}{C} d_k \ln p_k
                    \label{rn_equation_vraisemblance_kullbck_leiber}
    \end{eqnarray}

Cette fonction est aussi appelée distance de Kullback-Leiber (\citeindex{Kullback1951}), elle mesure la distance entre deux distributions de variables aléatoires discrètes. L'estimateur de maximum de vraisemblance (emv) \indexfr{emv}\indexfr{estimateur} est la solution du problème suivant~:

		\begin{xproblem}{estimateur du maximum de vraisemblance} \label{problem_emv}
		\indexfr{emv}
		Soit un vecteur $\vecteur{d_1}{d_N}$ tel que~:
		    $$
		    \left\{
		    \begin{array}{l}
		    \summy{k=1}{N} d_k = 1 \\
		    \forall k \in \ensemble{1}{N}, \; d_k \supegal 0 
		    \end{array}
		    \right.
		    $$
		
		On cherche le vecteur $\vecteur{p_1^*}{p_N^*}$ vérifiant~:
		
		    $$
		    \begin{array}{l}
		    \vecteur{p_1^*}{p_N^*} = \underset{ \vecteur{p_1}{p_C} \in \R^C }{\arg \max} \summy{k=1}{C} d_k \ln p_k \medskip \\
		    \quad \text{avec } \left \{
		        \begin{array}{l}
		        \forall k \in \intervalle{1}{C}, \; p_k \supegal 0 \\
		        \text{et } \summy{k=1}{C} p_k = 1
		        \end{array}
		        \right.
		    \end{array}
		    $$
		
		\end{xproblem}



		\begin{xtheorem}{résolution du problème~\ref{problem_emv}} \label{theorem_problem_emv}
		La solution du problème~\ref{problem_emv} est le vecteur~:
		
		    $$
		    \vecteur{p_1^*}{p_N^*} = \vecteur{d_1}{d_N}
		    $$
		    
		\end{xtheorem}
		
		


\begin{xdemo}{théorème}{\ref{theorem_problem_emv}}

Soit un vecteur $\vecteur{p_1}{p_N}$ vérifiant les conditions~:

    $$
    \left\{
    \begin{array}{l}
    \summy{k=1}{N} p_k = 1 \\
    \forall k \in \ensemble{1}{N}, \;  p_k \supegal 0
    \end{array}
    \right.
    $$

La fonction $x \longrightarrow \ln x$ est concave, d'où~:

    \begin{eqnarray*}
    \Delta  &=&         \summy{k=1}{C} d_k \ln p_k - \summy{k=1}{C} d_k \ln d_k \\
            &=&         \summy{k=1}{C} d_k \pa{ \ln p_k - \ln d_k } = \summy{k=1}{C} d_k \ln \frac{p_k}{d_k} \\
            &\infegal&  \ln \pa{ \summy{k=1}{C} d_k \frac{p_k}{d_k} } = \ln \pa { \summy{k=1}{C} p_k } = \ln 1 = 0 \\
            &\infegal&  0
    \end{eqnarray*}

\end{xdemo}



La distance de KullBack-Leiber compare deux distributions de probabilités entre elles. C'est elle qui va faire le lien entre le problème de classification discret (\ref{problem_classification}) et les réseaux de neurones pour lesquels il faut impérativement une fonction d'erreur dérivable. 








\subsection{Problème de classification pour les réseaux de neurones}
\indexfr{classification}%


Le problème de classification~\ref{problem_classification} est un cas particulier de celui qui suit pour lequel il n'est pas nécessaire de connaître la classe d'appartenance de chaque exemple mais seulement les probabilités d'appartenance de cet exemple à chacune des classes.

Soient une variable aléatoire continue $ X \in \ensemblereel^p $ et une variable aléatoire discrète multinomiale $Y \in \intervalle{1}{C}$, on veut estimer la loi de~:

\indexfrr{loi}{multinomiale}%

    $$
    Y|X \sim \loimultinomiale {p_1\pa{W,X},\dots , p_C\pa{W,X}} \text { avec } W \in \ensemblereel^M
    $$

Le vecteur $\vecteur{p_1\pa{W,X}}{p_C\pa{W,X}}$ est une fonction $f$ de $\pa{W,X}$ où $W$ est l'ensemble des $M$ paramètres du modèle. Cette fonction possède $p$ entrées et $C$ sorties. Comme pour le problème de la régression, on cherche les poids $W$ qui correspondent le mieux à l'échantillon~:

    $$
    A = \accolade {\left. \pa {X_i,y_i=\pa{\eta_i^k}_{1 \infegal k \infegal C}} \in \R^p \times \cro{0,1}^C
                        \text{ tel que } \summy{k=1}{c}y_i^k=1 \right| 1 \infegal i \infegal N }
    $$


On suppose que les variables $\pa{Y_i|X_i}_{1 \infegal i \infegal N}$ suivent les lois respectives $\pa{\loimultinomiale{y_i}}_{1 \infegal i \infegal N}$ et sont indépendantes entre elles, la vraisemblance du modèle vérifie d'après l'équation (\ref{rn_equation_vraisemblance_kullbck_leiber})~: \indexfr{vraisemblance}%
\indexfr{logit}%

    \begin{eqnarray*}
    L_W & \propto & \prody{i=1}{N}\prody{k=1}{C} \crochet{p_k \pa{W,X_i}}^{\pr{Y_i=k}} \\
    \ln L_W & \propto & \summy{i=1}{N}\summy{k=1}{C} \eta_i^k \ln\crochet { p_k\pa{W,X_i}}
    \end{eqnarray*}

La solution du problème  $\overset{*}{W} = \underset{W \in \R^l}{\arg \max} \; L_W$ est celle d'un problème d'optimisation sous contrainte. Afin de contourner ce problème, on définit la fonction $f$~:

    $$
    \begin{array}{l}
    f : \R^M \times \R^p \longrightarrow \R^C \\
    \forall \pa{W,x} \in \R^M \times \R^p, \; f\pa{W,x} = \pa{f_1\pa{W,x}}, \dots , f_C\pa{W,x} \vspace{0.5ex}\\
    \text{et }\forall i \in \intervalle{1}{N}, \; \forall k \in \intervalle{1}{C}, \; 
    				p^k \pa{W,X_i} = \dfrac{e^{f_k\pa{W,X_i}}}
    {\summy{l=1}{C}e^{f_l\pa{W,X_i}}}
    \end{array}
    $$

Les contraintes sur $\pa{p^k\pa{W,X_i}}$ sont bien vérifiées~:

    $$
    \begin{array}{l}
    \forall i \in \intervalle{1}{N},\; \forall k \in \intervalle{1}{C}, \; p^k\pa{W,X_i} \supegal 0 \\
    \forall i \in \intervalle{1}{N},\; \summy{k=1}{C} p^k\pa{W,X_i} = 1
    \end{array}
    $$

On en déduit que~:

		\begin{eqnarray*}
		\ln L_W & \propto & \summy{i=1}{N}\summy{k=1}{C} \; \eta_i^k  \crochet { f_k\pa{W,X_i} - \ln 
		\crochet{\summy{l=1}{C}e^{f_l\pa{W,X_i}}}} \\
		\ln L_W & \propto & \summy{i=1}{N}\summy{k=1}{C} \; \eta_i^k  f_k\pa{W,X_i} -
		                  \summy{i=1}{N}  \ln \crochet{\summy{l=1}{C}e^{f_l\pa{W,X_i}}}
		                  \underset{=1}{\underbrace{\summy{k=1}{C} \eta_i^k}}
		\end{eqnarray*}

D'où~:

			\begin{eqnarray}
			    \begin{array}[c]{c}
			    \ln L_W \propto  \summy{i=1}{N} \summy{k=1}{C} \eta_i^k  f_k\pa{W,X_i} - \summy{i=1}{N} 
			     \ln \crochet{ \summy{l=1}{C} e^{f_l\pa{W,X_i} }}
			    \end{array}
			\label{nn_classification_vraisemblance_error}			    
			\end{eqnarray}


Ceci mène à la définition du problème de classification suivant~:


		\begin{xproblemmine}{classification}\label{problem_classification_2}
		\indexfr{classification}
		
		Soit $A$ l'échantillon suivant~:
		
		    $$
		    A = \accolade {\left. \pa {X_i,y_i=\pa{\eta_i^k}_{1 \infegal k \infegal C}} \in 
		    										\ensemblereel^p \times \ensemblereel^C
		                        \text{ tel que } \summy{k=1}{c}\eta_i^k=1 \right| 1 \infegal i \infegal N }
		    $$
		
		$y_i^k$ représente la probabilité que l'élément $X_i$ appartiennent à la classe $k$~:
		
		    $$
		    \eta_i^k = \pr{Y_i = k \sachant X_i}
		    $$
		
		Le classifieur cherché est une fonction $f$ définie par~:
		
		    $$
		    \begin{array}{rcl}
		    f : \R^M \times \R^p &\longrightarrow& \R^C \\
		    \pa{W,X}    &\longrightarrow&  \vecteur{f_1\pa{W,X}}{f_p\pa{W,X}} \\
		    \end{array}
		    $$
		
		%Vérifiant~:
		%		
		%    $$
		%    \forall \pa{W,X} \in \R^M \times \R^p, \; \left\{
		%        \begin{array}{l}
		%        f_1\pa{W,X} + ... + f_C\pa{W,X} = 1 \\
		%        \forall k \in \ensemble{1}{C}, \; f_k\pa{W,X} \supegal 0
		%        \end{array}
		%        \right.
		%    $$
		
		Dont le vecteur de poids $W^*$ est égal à~:
		
		    $$
		    W^* =   \underset{W}{\arg \max} \;
		            \summy{i=1}{N} \summy{k=1}{C} \eta_i^k  f_k\pa{W,X_i} - 
		            \summy{i=1}{N}  \ln \crochet{ \summy{l=1}{C} e^{f_l\pa{W,X_i} }}
		    $$
		
		\end{xproblemmine}
		
		
		
		
		





\subsection{Réseau de neurones adéquat}

\indexfrr{classification}{réseau de neurones adéquat}

Dans le problème précédent, la maximisation de $\overset{*}{W} = \underset{W \in \R^M}{\arg \max} \, L_W$ aboutit au choix d'une fonction~:

    $$
    X \in \R^p \longrightarrow f(\overset{*}{W},X) \in \R^C 
    $$

Le réseau de neurones (voir figure~\ref{figure_rn_classification_adequat_figure}) $g : \pa{W,X} \in \R^M \times \R^p \longrightarrow \R^C$ choisi pour modéliser $f$ aura pour sorties~:

    $$
    \begin{array}{l}
    X \in \R^p \longrightarrow g(\overset{*}{W},X) \in \R^C\\
    \forall k \in \intervalle{1}{C}, \; g_k \pa{W,X} = e^{f_k\pa{W,X}}
    \end{array}
    $$

    \begin{figure}[t]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=8cm, width=12cm]
    {\filext{../dessin2/rn_clad}}\end{array}$}$$
    \caption{Réseau de neurones adéquat pour la classification.}
    \label{figure_rn_classification_adequat_figure}
    \end{figure}


Les conséquences sont~:

\begin{enumerate}
\item la fonction de transert des neurones de la couche de sortie est : $x \longrightarrow e^x$
\item la probabilité pour le vecteur $x\in\R^p$ d'appartenir à la classe $k\in\intervalle{1}{C}$ est :
              $$
              p_k(\overset{*}{W},x) = \pr{Y=k|x} = \dfrac { g_k(\overset{*}{W},x)}
              			 {\summy {l=1}{C} g_l(\overset{*}{W},x) }
              $$
\item la fonction d'erreur à minimiser est l'opposé de la log-vraisemblance du modèle :
              \begin{eqnarray*}
              \overset{*}{W} &=& \underset{W \in \R^M}{\arg \min}
                      \crochet {\summy{i=1}{N} \pa { - \summy{k=1}{C} \eta_i^k  \ln \pa{g_k\pa{W,X_i}} +
                                    \ln \crochet{ \summy{l=1}{C} g_l\pa{W,X_i} }}} \\
                      &=& \underset{W \in \R^M}{\arg \min}  \crochet {\summy{i=1}{N} h\pa{W,X_i,\eta_i^k}}
              \end{eqnarray*}
              \end{enumerate}

On note $C_{rn}$ le nombre de couches du réseau de neurones, $z_{C_{rn}}^k$ est la sortie $k$ avec $k \in \intervalle{1}{C}$, $g_k\pa{W,x} = z_{C_{rn}}^k = e^{y_{C_{rn}}^k}$ où $y_{C_{rn}}^k$ est le potentiel du neurone $k$ de la couche de sortie.

On calcule~:

    \begin{eqnarray*}
    \partialfrac{h\pa{W,X_i,y_i^k}}{y_{C_{rn}}^k} &=& - \eta_i^k +  \dfrac{z_{C{rn}}^i}{\summy {m=1}{C}z_{C{rn}}^m} \\
    &=& p_k(\overset{*}{W},x) - \eta_i^k
    \end{eqnarray*}

\indexfr{classifieur}%

Cette équation permet d'adapter l'algorithme~\ref{algo_retropropagation} décrivant rétropropagation pour le problème de la classification et pour un exemple $\pa {X,y=\pa{\eta^k}_{1 \infegal k \infegal C}}$. Seule la couche de sortie change.


		\begin{xalgorithm}{rétropropagation} \label{algo_retropropagation_class}
		\indexfr{rétropropagation}%
		Cet algorithme de rétropropagation est l'adaptation de \ref{algo_retropropagation} pour le problème
		de la classification. Il suppose que l'algorithme de propagation (\ref{algo_propagation}) 
		a été préalablement exécuté.\newline%
		On note $y'_{c,i} = \partialfrac{e}{y_{c,i}}$, $w'_{c,i,j} = \partialfrac{e}{w_{c,i,j}}$ et $b'_{c,i} =
		\partialfrac{e}{b_{c,i}}$. \medskip
		
		\begin{xalgostep}{initialisation}
		    \begin{xfor}{i}{1}{C_C}
		    $y'_{C,i} \longleftarrow \dfrac{z_{C,i}} {\summy{l=1}{C} z_{C,l} } - \eta_i$
		    \end{xfor}
		\end{xalgostep}
		
		\begin{xalgostep}{récurrence}
		    voir étape~\ref{algo_retropropagation_recurrence} et l'algorithme~\ref{algo_retropropagation}
		\end{xalgostep}
		
		\begin{xalgostep}{terminaison}
		    voir étape~\ref{algo_retropropagation_terminaison} et l'algorithme~\ref{algo_retropropagation}
		\end{xalgostep}
		
		\end{xalgorithm}




\begin{xremark}{nullité du gradient}
On vérifie que le gradient s'annule lorsque le réseau de neurones retourne pour l'exemple $\pa{X_i,y_i}$ la
distribution de $Y|X_i \sim \loimultinomiale{y_i}$.
\end{xremark}

\begin{xremark}{probabilité de sortie}
L'algorithme de rétropropagation~\ref{algo_retropropagation_class} utilise un vecteur désiré de probabilités $\vecteur{\eta_1}{\eta_{C_C}}$ vérifiant $\sum_{i=1}^{C_C} \, \eta_i = 1$. L'expérience montre qu'il est préférable d'utiliser un vecteur vérifiant la contrainte~:
\label{nn_remark_classification_output_alpha}
\indexfrr{probabilité}{sortie}

				\begin{eqnarray}
				&& \forall i \in \ensemble{1}{C_C}, \;  \min\acc{ \eta_i, 1-\eta_i} > \alpha \\
				&& \text{avec } \alpha > 0 \nonumber
				\end{eqnarray}

Généralement, $\alpha$ est de l'ordre de $0,1$ ou $0,01$. Cette contrainte facilite le calcul de la vraisemblance (\ref{nn_classification_vraisemblance_error}) et évite l'obtention de gradients quasi-nuls qui freinent l'apprentissage lorsque les fonctions exponnetielles sont saturées (voir \citeindex{Bishop1995}). 
\end{xremark}















%--------------------------------------------------------------------------------------------------------------------
\section{Prolongements}
%--------------------------------------------------------------------------------------------------------------------






\subsection{Base d'apprentissage et base de test}

\indexfrr{apprentissage}{base}%
\indexfrr{base}{test}%
\indexfrr{base}{apprentissage}%



Les deux exemples de régression et de classification (paragraphes~\ref{rn_section_regression} et~\ref{subsection_classifieur} ) ont montré que la structure du réseau de neurones la mieux adaptée a une grande importance. Dans ces deux cas, une rapide vérification visuelle permet de juger de la qualité du modèle obtenu après apprentissage, mais bien souvent, cette "vision" est inaccessible pour des dimensions supérieures à deux. Le meilleur moyen de jauger le modèle appris est de vérifier si l'erreur obtenue sur une base ayant servi à l'apprentissage (ou \emph{base d'apprentissage}) est conservée sur une autre base (ou \emph{base de test}) que le modèle découvre pour la première fois.

Soit $B=\accolade{\pa{X_i,Y_i} \sachant 1 \infegal i \infegal N}$ l'ensemble des observations disponibles. Cet ensemble est aléatoirement scindé en deux sous-ensembles $B_a$ et $B_t$ de telle sorte que :

    $$
    \begin{array}{l}
    B_a \neq \emptyset \text{ et } B_t \neq \emptyset \\
    B_a \cup B_t = B \text{ et } B_a \cap B_t = \emptyset \\
    \frac{\card{B_a}}{\card{B_a \cup B_t}} = p \in \crochetopen{0,1} 
    			\text{, en règle générale, } p \in \crochet{\frac{1}{2},\frac{3}{4}}
    \end{array}
    $$

Ce découpage est valide si tous les exemples de la base $B$ obéissent à la même loi, les deux bases $B_a$ et $B_t$ sont dites \emph{homogènes}. \indexfrr{base}{homogène} Le réseau de neurones sera donc appris sur la base d'apprentissage $B_a$ et "testé" sur la base de test $B_t$. Le test consiste à vérifier que l'erreur sur $B_t$ est sensiblement égale à celle sur $B_a$, auquel cas on dit que le modèle (ou réseau de neurones) généralise bien. \indexfr{généralisation} Le modèle trouvé n'est pas pour autant le bon modèle mais il est robuste. La courbe figure~\ref{figure_modele_optimal} illustre une définition du modèle optimal comme étant celui qui minimise l'erreur sur la base de test. Lorsque le modèle choisi n'est pas celui-là, deux cas sont possibles~:

\begin{enumerate}
\item Le nombre de coefficients est trop petit~: le modèle généralise bien mais il existe d'autres modèles meilleurs pour lesquels l'erreur d'apprentissage et de test est moindre.
\item Le nombre de coefficients est trop grand~: le modèle généralise mal, l'erreur d'apprentissage est faible et l'erreur de test élevée, le réseau a appris la base d'apprentissage par c\oe ur.
\end{enumerate}

		\begin{figure}[ht]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=6cm, width=12cm]
    {\filext{../dessin2/errapptest}}\end{array}$}$$
    \caption{Modèle optimal pour la base de test}
    \label{figure_modele_optimal}
		\end{figure}

Ce découpage des données en deux bases d'apprentissage et de test est fréquemment utilisé pour toute estimation de modèles résultant d'une optimisation réalisée au moyen d'un algorithme itératif. C'est le cas par exemple des modèles de Markov cachés\seeannex{annexe_hmm_def}{modèles de Markov cachés}. Elle permet de s'assurer qu'un modèle s'adapte bien à de nouvelles données.




\subsection{Fonction de transfert à base radiale}
\label{rnn_fonction_base_radiale_rbf}
\indexfr{RBF}
\indexsee{Radial basis function}{RBF}
\indexsee{fonction à base radiale}{RBF}
\indexfrr{fonction}{transfert} 

La fonction de transfert est dans ce cas à base radiale (souvent abrégée par RBF pour "radial basis function"). Elle ne s'applique pas au produit scalaire entre le vecteur des poids et celui des entrées mais à la distance euclidienne entre ces vecteurs.


		\begin{xdefinition}{neurone distance}
		\label{rn_definition_neurone_dist}
		\indexfrr{neurone}{distance}%
		Un neurone distance à $p$ entrées est une fonction 
		$f : \R^{p+1} \times \R^p \longrightarrow \R$ définie par~:
		\begin{enumerate}
		    \item $g : \R \dans \R$
		    \item $W \in \R^{p+1}$, $W=\pa{w_1,\dots,w_{p+1}} = \pa{W',w_{p+1}}$
		    \item $\forall x \in \R^p, \; f\pa{W,x} = e^{-\norme{W'-x}^2 + w_{p+1}}$ \newline
		        avec $x = \pa{x_1,\dots,x_p}$
		\end{enumerate}
		\end{xdefinition}


Ce neurone est un cas particulier du suivant qui pondère chaque dimension par un coefficient. Toutefois, ce neurone possède $2p+1$ coefficients où $p$ est le nombre d'entrée.


		\begin{xdefinition}{neurone distance pondérée}
		\label{rn_definition_neurone_dist_pond}
		\indexfrr{neurone}{distance pondérée}%
		Pour un vecteur donné $W \in \R^p = \pa{w_1,\dots,w_p}$, on note $W_i^j = \pa{w_i,\dots,w_j}.$
		Un neurone distance pondérée à $p$ entrées est une fonction 
		$f : \R^{2p+1} \times \R^p \longrightarrow \R$ définie par~:
		\begin{enumerate}
		    \item $g : \R \dans \R$
		    \item $W \in \R^{2p+1}$, $W=\pa{w_1,\dots,w_{2p+1}} = \pa{w_1,w_{2p+1}}$
		    \item $\forall x \in \R^p, \; f\pa{W,x} = 
		    		\exp \cro {-\cro{\summy{i=1}{p} w_{p+i}\pa{w_i - x_i}^2 } + w_{p+1}}$ \newline
		        avec $x = \pa{x_1,\dots,x_p}$
		\end{enumerate}
		\end{xdefinition}


La fonction de transfert est $x \longrightarrow e^x$ est le potentiel de ce neurone donc~:
    $$
    y = -\cro{\summy{i=1}{p} w_{p+i}\pa{w_i - x_i}^2 } + w_{p+1}
    $$

L'algorithme de rétropropagation~\ref{algo_retropropagation}\indexfr{rétropropagation} est modifié par l'insertion d'un tel neurone dans un réseau ainsi que la rétropropagation. Le plus simple tout d'abord~:

    \begin{eqnarray}
    1 \infegal i \infegal p, & \dfrac{\partial y}{\partial w_{i}} = & - 2 w_{p+i}\pa{w_i - x_i} \\  
    p+1 \infegal i \infegal 2p, & \dfrac{\partial y}{\partial w_{i}} = & - \pa{w_i - x_i}^2 \label{eq_no_distance_nn} \\  
    i = 2p+1, & \dfrac{\partial y}{\partial w_{i}} = & -1
    \end{eqnarray}
    
Pour le neurone distance simple, la ligne (\ref{eq_no_distance_nn}) est superflue, tous les coefficients $(w_i)_{p+1 \infegal i \infegal 2p}$ sont égaux à~1. La relation (\ref{retro_eq_nn_3}) reste vraie mais n'aboutit plus à (\ref{algo_retro_5}), celle-ci devient en supposant que la couche d'indice~$c+1$ ne contient que des neurones définie par~(\ref{rn_definition_neurone_dist_pond})~:

    \begin{eqnarray}
    \partialfrac{e}{y_{c,i}}  
                                &=& \sum_{l=1}^{C_{c+1}}              \partialfrac{e}{y_{c+1,l}}
                                                                    \partialfrac{y_{c+1,l}}{z_{c,i}}
                                                                    \partialfrac{z_{c,i}}{y_{c,i}} \nonumber \\
         &=& \cro{ \sum_{l=1}^{C_{c+1}}              
         						\partialfrac{e}{y_{c+1,l}}
                    \pa{ 2 w_{c+1,l,p+i} \pa{ w_{c+1,l,i} - z_{c,i} } } }
                    \partialfrac{z_{c,i}}{y_{c,i}} 
    \end{eqnarray}












\subsection{Poids partagés}



\indexfrr{neurone}{poids partagés}%

Les poids partagés sont simplement un ensemble de poids qui sont contraints à conserver la même valeur. Soit $G$ un groupe de poids partagés dont la valeur est $w_{G}$. Soit $X_k$ et $Y_k$ un exemple de la base d'apprentissage (entrées et sorties désirées), l'erreur commise par le réseau de neurones est $e\left(  W,X_k,Y_k\right)$.

    $$
    \dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }
    {\partial w_{G}}=\underset{w\in G}{\sum}\dfrac{\partial e\left(  W,X_{k},Y_{k}\right) }{\partial
    w_G}\dfrac{\partial w_{G}}{\partial w}=\underset{w\in G}
    {\sum} \dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial w_G}
    $$

Par conséquent, si un poids $w$ appartient à un groupe $G$ de poids partagés, sa valeur à l'itération suivante sera~:

    $$
    w_{t+1}=w_{t}-\varepsilon_{t}\left(  \underset{w\in G}
    {\sum}\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial w}\right)
    $$















\subsection{Dérivée par rapport aux entrées}

\indexfrr{neurone}{entrée}
\indexfrr{gradient}{entrée}

On note $\left(  X_k,Y_k\right)  $ un exemple de la base d'apprentissage. Le réseau de neurones est composé de $C$ couches, $C_i$ est le nombre de neurones sur la $i^{ième}$ couche, $C_0$ est le nombre d'entrées. Les entrées sont appelées $\left( z_{0,i}\right) _{1\leqslant i\leqslant C_{0}}$, $\left(  y_{1,i}\right)  _{1\leqslant i\leqslant C_{1}}$ sont les potentiels des neurones de la première couche, on en déduit que, dans le cas d'un neurone classique (non distance) :%

		$$
		\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial z_{0,i}} =
			\underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left(  W,X_{k}
		,Y_{k}\right)  }{\partial y_{1,j}}\dfrac{\partial y_{1,j}}{\partial z_{0,i}
		 }=\underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left( W,X_{k}
		,Y_{k}\right)  }{\partial y_{1,j}}w_{1,j,i}
		$$

Comme le potentiel d'un neurone distance n'est pas linéaire par rapport aux entrées $\left( y=\overset{N} {\underset{i=1}{\sum}}\left( w_{i}-z_{0,i}\right)  ^{2}+b\right)  $, la formule devient dans ce cas~:%

		$$
		\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial z_{0,i}} =
				\underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left(  W,X_{k}
		,Y_{k}\right)  }{\partial y_{1,j}}\dfrac{\partial y_{1,j}}{\partial z_{0,i}
			 }=-2\underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left(
		W,X_{k},Y_{k}\right)  }{\partial y_{1,j}}\left(  w_{1,j,i}-z_{0,i}\right)
		$$








\subsection{Régularisation ou Decay} \label{rn_decay}
\indexfr{decay}
\indexfr{régularisation}

Lors de l'apprentissage, comme les fonctions de seuil du réseau de neurones sont bornées, pour une grande variation des coefficients, la sortie varie peu. De plus, pour ces grandes valeurs, la dérivée est quasi nulle et l'apprentissage s'en trouve ralenti. Par conséquent, il est préférable d'éviter ce cas et c'est pourquoi un terme de régularisation est ajouté lors de la mise à jour des coefficients (voir~\citeindex{Bishop1995}). L'idée consiste à ajouter à l'erreur une pénalité fonction des coefficients du réseau de neurones~:

			\begin{eqnarray}
			E_{reg} = E + \lambda \; \summy{i} \; w_i^2
			\end{eqnarray}

Et lors de la mise à jour du poids $w_i^t$ à l'itération $t+1$~:

			\begin{eqnarray}
			w_i^{t+1} &=& w_i^t - \epsilon_t \cro{ \partialfrac{E}{w_i} - 2\lambda w_i^t }
			\end{eqnarray}

Le coefficient $\lambda$ peut décroître avec le nombre d'itérations et est en général de l'ordre de $0,01$ pour un apprentissage avec gradient global, plus faible pour un apprentissage avec gradient stochastique.

















%--------------------------------------------------------------------------------------------------------------------
\section{Sélection de connexions}
%--------------------------------------------------------------------------------------------------------------------

\label{selection_connexion}
\indexfrr{sélection}{architecture}
\indexfrr{architecture}{sélection}

Ce paragraphe présente un algorithme de sélection de l'architecture d'un réseau de neurones proposé par Cottrel et Al. dans \citeindex{Cottrel1995}. La méthode est applicable à tout réseau de neurones mais n'a été démontrée que pour la classe de réseau de neurones utilisée pour la régression (paragraphe~\ref{regression}, page~\pageref{regression}). Les propriétés qui suivent ne sont vraies que des réseaux à une couche cachée et dont les sorties sont linéaires. Soit $\pa{X_k,Y_k}$ un exemple de la base d'apprentissage, les résidus de la régression sont supposés normaux et i.i.d. \indexfr{i.i.d.}\indexfrr{loi}{normale}\indexfr{résidus} L'erreur est donc (voir paragraphe~\ref{rn_enonce_probleme_regression})~:

    $$
    e\left( W,X_k,Y_k\right) =\left(f\left( W,X_k\right)  -Y_k\right)^2
    $$

On peut estimer la loi asymptotique des coefficients du réseau de neurones. Des connexions ayant un rôle peu important peuvent alors être supprimées sans nuire à l'apprentissage en testant la nullité du coefficient associé. On note $\widehat{W}$ les poids trouvés par apprentissage et $\overset{\ast}{W}$ les poids optimaux. On définit~:

    \begin{eqnarray}
    \text{la suite } \widehat{\varepsilon_{k}} &=&   f\left(  \widehat{W} ,X_{k}\right)  -Y_{k}, \;
    							 \widehat{\sigma}_{N}^{2}=\dfrac{1}{N}\underset
                                    {k=1}{\overset{N}{\sum}}\widehat{\varepsilon_{k}}^{2} \label{rn_selection_suite}\\
    \text{la matrice }
    \widehat{\Sigma_{N}}      &=&   \dfrac{1}{N}\left[  \nabla_{\widehat{W}%
                                    }e\left(  W,X_{k},Y_{k}\right)  \right]  
                                    \left[  \nabla_{\widehat{W}}
                                    e\left(  W,X_{k},Y_{k}\right)  \right]  ^{\prime} \label{rn_selection_matrice}
    \end{eqnarray}



		\begin{xtheorem}{loi asymptotique des coefficients} \label{theoreme_loi_asym}
		\indexfrr{loi}{asymptotique}
		Soit $f$ un réseau de neurone défini par (\ref{rn_definition_perpception_1}) composé de~:
		\begin{itemize}
		\item une couche d'entrées
		\item une couche cachée dont les fonctions de transfert sont sigmoïdes \indexfrr{fonction}{sigmoïde}
		\item une couche de sortie dont les fonctions de transfert sont linéaires \indexfrr{fonction}{linéaire}
		\end{itemize}
		
		Ce réseau sert de modèle pour la fonction $f$ dans le problème~\ref{problem_regression} avec un échantillon
		$\vecteur{\pa{X_1,Y_1}}{\pa{X_N,Y_N}}$, les résidus sont supposés normaux.
		
		La suite $\pa{\widehat{\epsilon_k}}$ définie par (\ref{rn_selection_suite}) vérifie~:
		    $$
		    \dfrac{1}{N} \summy{i=1}{N} \widehat{\epsilon_k} = 0 = \esperance {f\pa{\widehat{W},X} - Y}
		    $$
		
		Et le vecteur aléatoire  $\widehat{W} - W^*$ vérifie~:
		
		    $$
		    \sqrt{N} \cro { \widehat{W} - W^* } \; \overset{T \rightarrow + \infty}{\longrightarrow} \;
		            \loinormale{0}{\widehat{\sigma_N}^2  \widehat{\Sigma_N}}
		    $$
		Où la matrice $\widehat{\Sigma_N}$ est définie par (\ref{rn_selection_matrice}).
		
		\end{xtheorem}


		\begin{figure}[t]
    $$\frame{$\begin{array}[c]{c}\includegraphics[height=9cm, width=14cm] 
    {\filext{../dessin2/selection_connexion}}\end{array}$}$$
    \caption{Réseau de neurones pour lequel la sélection de connexions s'applique.}
    \label{figure_selection_connexion_reseau-fig}
		\end{figure}


La démonstration de ce théorème est donnée par l'article \citeindex{Cottrel1995}. Ce théorème mène au corollaire suivant~:

		\begin{xcorollary}{nullité d'un coefficient}
		\indexfrr{loi}{normale}
		\indexfrr{loi}{$\chi_2$}
		\indexfrr{test}{$\chi_2$}
		\indexfrr{test}{statistique}
		
		Les notations utilisées sont celles du théorème~\ref{theoreme_loi_asym}. Soit $w_k$ un poids du réseau de neurones
		d'indice quelconque $k$. Sa valeur estimée est $\widehat{w_k}$, sa valeur optimale $w^*_k$. D'après
		le théorème~\ref{theoreme_loi_asym}~:
		
		    $$
		    N \dfrac{ \pa{\widehat{w_k} - w^*_k}^2  } { \widehat{\sigma_N}^2 \pa{\widehat{\Sigma_N}^{-1}}_{kk} }
		    \; \overset{T \rightarrow + \infty}{\longrightarrow} \; \chi^2_1
		    $$
		
		\end{xcorollary}


Ce résultat permet, à partir d'un réseau de neurones, de supprimer les connexions pour lesquelles l'hypothèse de nullité n'est pas réfutée. Afin d'aboutir à l'architecture minimale adaptée au problème, Cottrel et Al. proposent dans \citeindex{Cottrel1995} l'algorithme suivant~:


		\begin{xalgorithm}{sélection d'architecture}
		\label{rn_algorithme_selection_connexion_1}%
		\indexfrr{sélection}{architecture}
		\indexfrr{architecture}{sélection}
		Les notations utilisées sont celles du théorème~\ref{theoreme_loi_asym}. $f$ est un réseau de neurones
		de paramètres $W$. On définit la constante $\tau$, en général $\tau = 3,84$ puisque 
		$\pr {X < \tau} = 0,95$ si $X \sim \chi_1^2$.
		
		\begin{xalgostep}{initialisation}
		    Une architecture est choisie pour le réseau de neurones $f$ incluant un nombre~$M$ de paramètres.
		\end{xalgostep}
		
		\begin{xalgostep}{apprentissage}\label{algo_selection_apprentissage}
		    Le réseau de neurones $f$ est appris. On calcule les nombre et matrice 
		    $\widehat{\sigma_N}^2$ et $\widehat{\Sigma_N}$. La base d'apprentissage contient $N$ exemples.
		\end{xalgostep}
		
		\begin{xalgostep}{test}
		    \begin{xfor}{k}{1}{M}
		    $t_k \longleftarrow N \dfrac{ \widehat{w_k} ^2  } 
		    	{ \widehat{\sigma_N}^2 \pa{\widehat{\Sigma_N}^{-1}}_{kk} }$
		    \end{xfor}
		\end{xalgostep}
		
		\begin{xalgostep}{sélection}\label{algo_selection_selection}
		    $k' \longleftarrow \underset{k}{\arg \min} \; t_k$ \\
		    \begin{xif}{$t_{k'} < \tau$}
		        Le modèle obtenu est supposé être le modèle optimal. L'algorithme s'arrête.
		    \xelse
		        La connexion $k'$ est supprimée ou le poids $w_{k'}$ est maintenue à zéro. \newline%
		        $M \longleftarrow M-1$ \newline%
		        Retour à l'étape~\ref{algo_selection_apprentissage}.
		    \end{xif}
		\end{xalgostep}
		
		\end{xalgorithm}



\begin{xremark}{minimum local}
Cet algorithme est sensible au minimum local trouvé lors de l'apprentissage, il est préférable d'utiliser des méthodes
du second ordre afin d'assurer une meilleure convergence du réseau de neurones.
\end{xremark}

\begin{xremark}{suppression de plusieurs connexions simultanément}
L'étape~\ref{algo_selection_selection} ne supprime qu'une seule connexion. Comme l'étape~\ref{algo_selection_apprentissage}
est coûteuse en calcul, il peut être intéressant de supprimer toutes les connexions $k$ qui vérifient $t_k < \tau$. Il est toutefois conseillé de ne pas enlever trop de connexions simultanément puisque la suppression d'une connexion nulle peut
réhausser le test d'une autre connexion, nulle à cette même itération, mais non nulle à l'itération suivante.
\end{xremark}


\begin{xremark}{minimum local}
Dans l'article \citeindex{Cottrel1995}, les auteurs valident leur algorithme dans le cas d'une régression grâce à l'algorithme~\ref{nn_algorithme_valid_selection}.
\end{xremark}

		\begin{xalgorithm}{validation de l'algorithme~\ref{rn_algorithme_selection_connexion_1}}
		\label{nn_algorithme_valid_selection}
		\indexfr{validation}%
		
		\begin{xalgostep}{choix aléatoire d'un modèle}\label{algo_validation_init}
		    \begin{enumerate}
		    \item Un réseau de neurones est choisi aléatoirement, soit $f : \R^p \dans \R$ la fonction qu'il représente.
		    \item Une base d'apprentissage $A$ (ou échantillon) 
		    			de $N$ observations est générée aléatoirement à partir de ce modèle~:
		        \indexfrr{loi}{normale}%
		        \indexfrr{loi}{bruit blanc}%
		        
		        $$
		        \begin{array}{l}
		        \text{soit } \pa{\epsilon_i}_{1 \infegal i \infegal N} \text{ un bruit blanc 
		        					(voir définition~\ref{bruitblanc})} \\
		        A = \accolade{ \left. \pa{X_i,Y_i}_{1 \infegal i \infegal N} \right| 
		        			\forall i \in \intervalle{1}{N}, \; Y_i = f\pa{X_i} + \epsilon_i }
		        \end{array}
		        $$
		    \end{enumerate}
		\end{xalgostep}
		
		\begin{xalgostep}{choix aléatoire d'un modèle}
		    L'algorithme~\ref{rn_algorithme_selection_connexion_1} à un réseau de neurone plus riche que le modèle choisi
		    dans l'étape~\ref{algo_validation_init}. Le modèle sélectionné est noté $g$.
		\end{xalgostep}
		
		\begin{xalgostep}{validation}
		    Si $\norme{f-g} \approx 0$, l'algorithme~\ref{rn_algorithme_selection_connexion_1} est validé.
		\end{xalgostep}
		
		\end{xalgorithm}
		
		
		
		
		











%--------------------------------------------------------------------------------------------------------------------
\section{Analyse en composantes principales (ACP)} \label{ACP}
%--------------------------------------------------------------------------------------------------------------------






\indexfr{ACP}%
\indexsee{analyse en composantes principales}{ACP}

Cet algorithme est proposé dans \citeindex{Song1997}.



\subsection{Principe}

\indexfrr{réseau}{diabolo}%
\indexfr{diabolo}%
\indexfr{projection}%
\indexfr{compression}%

L'algorithme implémentant l'analyse en composantes principales est basé sur un réseau linéaire dit "diabolo", ce réseau
possède une couche d'entrées à $N$ entrées, une couche cachée et une couche de sortie à $N$ sorties. L'objectif est
d'apprendre la fonction identité sur l'espace $\R^N$. Ce ne sont plus les sorties qui nous intéressent mais la couche
cachée intermédiaire qui effectue une compression ou projection des vecteurs d'entrées puisque les entrées et les
sorties du réseau auront pour but d'être identiques (voir figure~\ref{figure_rn_acp-fig}). La
figure~\ref{figure_rn_acp-exemple} illustre un exemple de compression de vecteur de $\R^3$ dans $\R^2$.


		\begin{figure}[ht]
    \[
    \begin{tabular}{|c|}
    \hline
    \filefig{../rn/fig_rn_03}
    \\ \hline
    \end{tabular}
    \]
    \caption{Principe de la compression par un réseau diabolo}
    \label{figure_rn_acp-fig}
		\end{figure}


		\begin{figure}[ht]
    \[
    \begin{tabular}{|c|c|} \hline
        \begin{minipage}{9cm}
        Le réseau suivant possède 3 entrées et 3 sorties. Minimiser l'erreur~:
            $$
            \underset{k=1}{\overset{N}{{\displaystyle\sum}}}E\left(  X_{k},X_{k}\right)
            $$
        revient à compresser un vecteur de dimension 3 en un vecteur de dimension 2. Les coefficients de la
        première couche du réseau de neurones permettent de compresser les données. 
        Les coefficients de la seconde couche permettent de les décompresser.
        \end{minipage}
        &
				\filefig{../rn/fig_rn_04}        
    \\ \hline
    \end{tabular}
    \]
    \caption{Réseau diabolo : réduction d'une dimension}
    \label{figure_rn_acp-exemple}
		\end{figure}



La compression et décompression ne sont pas inverses l'une de l'autre, à moins que l'erreur (\ref{rn_equation_acp_error}) soit nulle. La décompression s'effectue donc avec des pertes d'information. L'enjeu de l'ACP est de trouver un bon compromis entre le nombre de coefficients et la perte d'information tôlérée. Dans le cas de l'ACP, la compression est "linéaire", c'est une projection.








\subsection{Problème de l'analyse en composantes principales}
\label{par_ACP_un}


L'analyse en composantes principales ou ACP \indexfr{ACP} est définie de la manière suivante~:

		\begin{xproblem}{analyse en composantes principales (ACP)}
		\label{problem_acp} 
		\indexfr{ACP}
		Soit $\pa{X_i}_{1 \infegal i \infegal N}$ avec $\forall i \in \ensemble{1}{N}, \; X_i \in \R^p$.\newline%
		Soit $W \in M_{p,d}\pa{\R}$, $W = \vecteur{C_1}{C_d}$ où les vecteurs $\pa{C_i}$ 
		sont les colonnes de $W$ et $d < p$.
		On suppose également que les $\pa{C_i}$ forment une base othonormée.
		\indexfrr{famille}{base}\indexfr{orthonormée} Par conséquent~:
		
		    $$
		    W'W = I_d
		    $$
		
		$\pa{W'X_i}_{1 \infegal i \infegal N}$ est l'ensemble des vecteurs $\pa{X_i}$ projetés sur le sous-espace vectoriel
		engendré par les vecteurs $\pa{C_i}$.
		
		Réaliser une analyse en composantes principales, c'est trouver le meilleur plan de projection pour les vecteurs
		$\pa{X_i}$, celui qui maximise l'inertie de ce nuage de points, c'est donc trouver $W^*$ tel que~:
		
		    \begin{eqnarray}
		    W^* &=& \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } 
		    									{ \arg \max } \; E\pa{W}
		        =  \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \arg \max } \;
		                        \cro { \summy{i=1}{N} \norme{W'X_i}^2 } \label{rn_equation_acp_error}
		    \end{eqnarray}
		
		Le terme $E\pa{W}$ est l'inertie du nuage de points $\pa{X_i}$ projeté sur le sous-espace vectoriel défini par les
		vecteurs colonnes de la matrice $W$.
		
		\end{xproblem}
		
		






\subsection{Résolution d'une ACP avec un réseau de neurones diabolo}

Un théorème est nécessaire avant de construire le réseau de neurones menant à la résolution du problème~\ref{problem_acp} afin de passer d'une optimisation sous contrainte à une optimisation sans contrainte. 

\indexfrr{optimisation}{contrainte}


		\begin{xtheoremmine}{résolution de l'ACP}
		\label{theorem_acp_resolution}
		\indexfr{optimisation}%
		Les notations utilisées sont celles du problème~\ref{problem_acp}. Dans ce cas~:
		
		    \begin{eqnarray}
		    S =
		    \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \arg \max } \;
		                        \cro { \summy{i=1}{N} \norme{W'X_i}^2 } &=&
		    \underset{ W \in M_{p,d}\pa{\R} } { \arg \min } \;  \cro { \summy{i=1}{N} \norme{WW'X_i - X_i}^2 }
		    \label{rn_acp_contrainte}
		    \end{eqnarray}
		
		De plus $S$ est l'espace vectoriel engendré par les~$d$ vecteurs propres de la matrice 
		$XX' = \summy{i=1}{N} X_i X_i'$ associés aux $d$ valeurs propres de plus grand module. 
		\indexfr{valeur propre}
		\indexfr{vecteur propre}
		\indexfr{module}
		
		\end{xtheoremmine}



\begin{xdemomine}{théorème}{\ref{theorem_acp_resolution}}

\itemdemo

L'objectif de cette partie est de chercher la valeur de~:

    $$
    \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \max }\; E\pa{W}
    $$

Soit $X=\vecteur{X_1}{X_N} \in \pa{\R^p}^N$, alors~:
    $$
    E\pa{W} = \summy{i=1}{N} \norme{W'X_i}^2 = \trace{X'WW'X} = \trace{XX'WW'}
    $$

La matrice $XX'$ est symétrique, elle est donc diagonalisable et il existe une matrice $P \in M_p\pa{\R}$ telle que~:

    \begin{eqnarray}
    \begin{array}{l}
    P'XX'P = D_X \text{ avec } D_X \text{ diagonale} \\
    P'P = I_p
    \end{array}
    \label{acp_equation_memo_1}
    \end{eqnarray}

Soit $P = \vecteur{P_1}{P_p}$ les vecteurs propres de la matrice $XX'$ associés aux valeurs propres
$\vecteur{\lambda_1}{\lambda_p}$ telles que $\abs{\lambda_1} \supegal ... \supegal \abs{\lambda_p}$. Pour mémoire, $W =
\vecteur{C_1}{C_d}$, et on a~:

    $$
    \begin{array}{l}
    \forall i \in \ensemble{1}{p}, \; XX'P_i = \lambda_i P_i \\
    \forall i \in \ensemble{1}{d}, \; C_i = P_i \Longrightarrow XX'WW' = D_{X,d} = \pa{
                                                        \begin{array}{ccc}
                                                        \lambda_1 & 0 & 0 \\
                                                        0  & \ldots & 0 \\
                                                        0 & 0 & \lambda_d
                                                        \end{array}
                                                        }
    \end{array}
    $$

D'où~:

    $$
    E\pa{W} = \trace{ XX'WW' } = \trace{P D_X P' WW'} = \trace{ D_X P'WW'P }
    $$

Donc~:

    \begin{eqnarray}
    \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \max }\; E\pa{W} = %
            \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \max }\; 
            	\trace{ D_X P'WW'P }
    = \underset{ \begin{subarray}{c} Y \in M_{p,d}\pa{\R} \\ Y'Y = I_d \end{subarray} } { \max }\; \trace{ D_X YY'
                }
    = \summy{i=1}{d} \lambda_i
    \label{acp_demo_partie_a}
    \end{eqnarray}


\itemdemo



Soit $Y \in \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \max }\;
\trace{X'WW'X}$, $Y = \vecteur{Y_1}{Y_d} = \pa{y_i^k}_{ \begin{subarray}{c} 1 \infegal i \infegal d \\ 1 \infegal k
\infegal p \end{subarray} }$.

Chaque vecteur $Y_i$ est écrit dans la base $\vecteur{P_1}{P_p}$ définie en (\ref{acp_equation_memo_1})~:

    $$
    \forall i \in \ensemble{1}{d}, \; Y_i = \summy{k=1}{p} y_i^k P_p
    $$

Comme $Y'Y = I_d$, les vecteurs $\vecteur{Y_1}{Y_d}$ sont orthogonaux deux à deux et normés, ils vérifient donc~:

    $$
    \left\{
    \begin{array}{rl}
    \forall i \in \ensemble{1}{d},          & \summy{k=1}{p} \pa{y_i^k}^2 = 1 \\
    \forall \pa{i,j} \in \ensemble{1}{d}^2, & \summy{k=1}{p} y_i^k y_j^k = 0
    \end{array}
    \right.
    $$


De plus~:

    $$
    XX'YY' = XX' \pa{ \summy{i=1}{d} Y_i Y_i'} =   \summy{i=1}{d} XX' Y_i Y_i'
    $$

On en déduit que~:

    \begin{eqnarray*}
    \forall i \in \ensemble{1}{d}, \; XX' Y_i Y'_i
                &=& XX' \pa{ \summy{k=1}{p} y_i^k P_k }\pa{ \summy{k=1}{p} y_i^k P_k }' \\
                &=& \pa{ \summy{k=1}{p} \lambda_k y_i^k P_k }\pa{ \summy{k=1}{p} y_i^k P_k }'
    \end{eqnarray*}

D'où~:

    $$
    \forall i \in \ensemble{1}{d}, \; \trace{ XX' Y_i Y'_i} = \summy{k=1}{p} \lambda_k \pa{y_i^k}^2
    $$

Et~:

    \begin{eqnarray*}
    \trace{ XX' YY'} &=& \summy{i=1}{d} \summy{k=1}{p} \lambda_k \pa{y_i^k}^2 \\
    \trace{ XX' YY'} &=& \summy{k=1}{p} \lambda_k \pa {\summy{i=1}{d} \pa{y_i^k}^2} =
    				\summy{k=1}{p} \; \lambda_k
    \end{eqnarray*}

Ceci permet d'affirmer que~:

    \begin{eqnarray}
    Y \in \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \max }\;
                \trace{X'WW'X}  \Longrightarrow
    vect \vecteur{Y_1}{Y_d} = vect \vecteur{P_1}{P_d}
    \label{acp_demo_partie_b}
    \end{eqnarray}

Les équations (\ref{acp_demo_partie_a}) et (\ref{acp_demo_partie_b}) démontrent la seconde partie du
théorème~\ref{theorem_acp_resolution}.


\itemdemo

    \begin{eqnarray*}
    \underset{i=1}{\overset{n}{\sum}}\left\|  WW^{\prime}X_{i}-X_{i}\right\|^{2} &=&
    \underset{i=1}{\overset{n}{\sum}}\left\|
        \left(  WW^{\prime} -I_{N}\right)  X_{i}\right\|  ^{2} \\
    &=& tr\left(  X^{\prime}\left(  WW^{\prime }-I_{p}\right)  ^{2}X\right)  \\
    &=& tr\left(  XX^{\prime}\left(  \left( WW^{\prime}\right) ^{2}-2WW^{\prime}+I_{p}\right)  \right) \\
    &=& tr\left(  XX^{\prime}\left(  WW^{\prime}WW^{\prime}-2WW^{\prime}+I_{p}\right)  \right) \\
    &=& tr\left(  XX^{\prime}\left(  -WW^{\prime} +I_{p}\right)  \right) \\
    &=& -tr\left(  XX^{\prime}WW^{\prime}\right)  +tr\left(XX^{\prime}\right)
    \end{eqnarray*}

D'où :

    \begin{eqnarray}
    \underset{ \begin{subarray} \, W \in M_{p,d} \pa{\R} \\ 
    						W'W=I_d \end{subarray}} { \; \max \; } \;  \pa {  \summy{i=1}{N} \norme{ W'X_i}^2 }  =%
    \underset{ \begin{subarray} \, W \in M_{p,d} \pa{\R} \\ 
    						W'W=I_d \end{subarray}} { \; \min \; } \;  \pa {  \summy{i=1}{N} \norme{ WW'X_i - X_i}^2 }
    \label{acp_demo_partie_c}
    \end{eqnarray}


\itemdemo

$XX'$ est une matrice symétrique, elle est donc diagonalisable~:

    $$
    \exists P\in GL_N \pa{\R}  \text{ telle que } P'XX'P=D_p \text{ où } D_p \text{ est diagonale}
    $$

On en déduit que~:

    \begin{eqnarray*}
        \summy{i=1}{N} \norme{  WW' X_i - X_i }^2
    &=& \trace{ XX' \pa{ WW'-I_p }^{2} } \\
    &=& \trace{ PP' XX' PP' \pa{ WW'-I_p }^{2} } \\
    &=& \trace{ P D_p P' \pa{ WW'-I_p }^{2} } \\
    &=& \trace{ D_p \pa{ P'WW'P-I_p }^{2} } \\
    &=& \trace{ D_p \pa{ YY'-I_p }^{2} } \text{ avec } Y = P'W
    \end{eqnarray*}

D'où~:

    \begin{eqnarray}
    \underset{Y}{\arg\min}\accolade{ tr\left(  D_{p}\left( YY^{\prime}-I_{p}\right)  ^{2}\right)}  = \left\{  Y\in
    M_{Nd}\left( \R\right) \left|
        YY^{\prime}=I_{d}\right.  \right\}
    \label{acp_demo_partie_d}
    \end{eqnarray}


Finalement, l'équation (\ref{acp_demo_partie_d}) permet de démontrer la première partie du théorème~\ref{theorem_acp_resolution}, à savoir (\ref{rn_acp_contrainte})~:

    \begin{eqnarray*}
    S =
    \underset{ \begin{subarray}{c} W \in M_{p,d}\pa{\R} \\ W'W = I_d \end{subarray} } { \arg \max } \;
                        \cro { \summy{i=1}{N} \norme{W'X_i}^2 } &=&
    \underset{ W \in M_{p,d}\pa{\R} } { \arg \min } \;  \cro { \summy{i=1}{N} \norme{WW'X_i - X_i}^2 }
    \end{eqnarray*}



\end{xdemomine}











\subsection{Calcul de valeurs propres et de vecteurs propres}
\label{par_ACP_deux}%
\indexfr{valeur propre}%
\indexfr{vecteur propre}%

Le calcul des valeurs propres et des vecteurs propres d'une matrice fait intervenir un réseau diabolo composé d'une
seule couche cachée et d'une couche de sortie avec des fonctions de transfert linéaires. On note sous forme de matrice
$\left( W\right)  $ les coefficients de la seconde couche du réseau dont les biais sont nuls. On note $d$ le nombre de
neurones sur la couche cachée, et $p$ le nombre d'entrées.

    $$
    \forall i\in\left\{  1,...,d\right\}  ,\,y_{1,i}=\overset{p}{\underset
    {j=1}{\sum}}w_{ji}x_{j}%
    $$
    
Soit $X\in\R^{p}$ les entrées, $Y=\left(  y_{1,1},...,y_{1,d}\right)  \in\R^{d}$, on obtient que : $Y=W'X$.

Les poids de la seconde couche sont définis comme suit~:

    $$
    \forall\left( i,j\right)  \in\left\{  1,...,p\right\}  \times\left\{ 1,...,d\right\} \,w_{2,j,i}=w_{1,i,j}
    $$
Par conséquent, le vecteur des sorties $Z\in\R^{p}$ du réseau ainsi construit est : $Z=WW'X$

On veut minimiser l'erreur pour $\left(  X_{i}\right)  _{1\leqslant i\leqslant N}$~:

    $$
    E=\underset{i=1}{\overset{N}{\sum}}\left\|  WW'X_{i}-X_{i}\right\|  ^{2}
    $$

Il suffit d'apprendre le réseau de neurones pour obtenir~:

    $$
    W_{d}^{\ast}=\underset{W\in M_{pd}\left(  \R\right)  }
    {\arg\max }\,\underset{i=1}{\overset{N}{\sum}}\left\| WW'X_{i}-X_{i}\right\|
    ^{2}%
    $$

D'après ce qui précède, l'espace engendré par les vecteurs colonnes de $W$ est l'espace engendré par les $k$ premiers vecteurs propres de la matrice $XX^{\prime}=\left(  X_{1},...,X_{P}\right)  \left( X_{1},...,X_{P}\right)  ^{\prime}$ associés aux $k$ premières valeurs propres classées par ordre décroissant de module.

\indexfr{Schmidt}

On en déduit que $W_{1}^{\ast}$ est le vecteur propre de la matrice $M$ associée à la valeur propre de plus grand module. $W_{2}^{\ast}$ est l'espace engendré par les deux premiers vecteurs. Grâce à une orthonormalisation de Schmidt (voir définition~\ref{orthonormalisation_schmidt}), on en déduit à partir de $W_{1}^{\ast}$ et $W_{2}^{\ast}$, les deux premiers vecteurs propres. Par récurrence, on trouve l'ensemble des vecteurs propres de la matrice $XX^{\prime}$.


		\begin{xdefinition}{orthonormalisation de Schmidt} \label{orthonormalisation_schmidt}
		\indexfr{orthonormalisation de Schmidt}
		\indexfr{Schmidt}
		
		L'orthonormalisation de Shmidt :
		
		Soit $\left(  e_{i}\right)  _{1\leqslant i\leqslant N}$ une base de $\R^{p}$
		
		On définit la famille $\left(  \varepsilon_{i}\right)  _{1\leqslant i\leqslant p}$ par :
		    \begin{eqnarray*}
		    \varepsilon_{1} &=& \dfrac{e_{1}}{\left\| e_{1}\right\|}\\
		    \forall i \in \intervalle{1}{p}, \; \varepsilon_{i} &=& \dfrac{e_{i}-\overset{i-1}{\underset{j=1}
		    {\sum}}<e_{i},\varepsilon_{j}>\varepsilon_{j}}{\left\| 
		    			e_{i}-\overset {i-1}{\underset{j=1}{\sum}}<e_{i},\varepsilon_{j}>\varepsilon_{j}\right\| }
		    \end{eqnarray*}
		\end{xdefinition}
		
		

\begin{xremark}{dénominateur non nul}
$e_{i}-\overset{i-1}{\underset{j=1}{\sum}}<e_{i},\varepsilon_{j}>\varepsilon_{j}\neq0$ car $\forall k\in\left\{ 1,...,N\right\}  ,\; vect\left( e_{1},...,e_{k}\right)  =vect\left(  \varepsilon_{1} ,...,\varepsilon_{k}\right)  $
\end{xremark}


		\begin{xproperty}{base orthonormée}
		\indexfrr{famille}{base}
		\indexfr{orthonormée}
		La famille $\left(  \varepsilon_{i}\right)  _{1\leqslant i\leqslant p}$ est une base orthonormée de $\R^{p}$
		\end{xproperty}


L'algorithme qui permet de déterminer les vecteurs propres de la matrice $XX'$ définie par le
théorème~\ref{theorem_acp_resolution} est le suivant~:


		\begin{xalgorithm}{vecteurs propres}\label{algorithm_vecteur_propre}
		\indexfr{vecteur propre}%
		Les notations utilisées sont celles du théorème~\ref{theorem_acp_resolution}. On note $V^*_d$ la matrice des $d$
		vecteurs propres de la matrice $XX'$ associés aux $d$ valeurs propres de plus grands module.
		
		\begin{xfor2}{d}{1}{p}
		    Un réseau diabolo est construit avec les poids $W_d \in M_{p,d}\pa{\R}$ puis appris. 
		    Le résultat de cet apprentissage sont
		    les poids $W^*_d$. \newline
		    \begin{xif}{$d > 1$}
		        L'orthonormalisation de Schmit permet de déduire $V^*_d$ de $V^*_{d-1}$ et $W^*_d$.
		    \xelse
		        $V^*_d = W^*_d$
		    \end{xif}
		\end{xfor2}
		
		\end{xalgorithm}









\subsection{Analyse en Composantes Principales (ACP)}

\indexfr{ACP}%

L'analyse en composantes principales permet d'analyser une liste d'individus décrits par des variables. Comme exemple, il suffit de prendre les informations extraites du recensement de la population française qui permet de décrire chaque habitant par des variables telles que la catégorie socio-professionnelle, la salaire ou le niveau d'étude.

Soit $\left(  X_{1},...,X_{N}\right)  $ un ensemble de $N$ individus décrits par $p$ variables~:

    $$
    \forall i\in\left\{  1,...,N\right\},\;X_{i}\in\R^{p}
    $$
    
L'ACP consiste à projeter ce nuage de point sur un plan qui conserve le maximum d'information. Par conséquent, il
s'agit de résoudre le problème~:

    $$
    W^{\ast}=\underset{ \begin{subarray} \, W\in M_{p,d}\left(  \R\right)  \\ 
    W^{\prime }W=I_{d} \end{subarray}}{\arg\min}%
    \left(\underset{i=1}{\overset{N}{\sum}}\left\| W'X_{i}\right\|  ^{2}\right)  \text{ avec }d<N
    $$

Ce problème a été résolu dans les paragraphes~\ref{par_ACP_un} et~\ref{par_ACP_deux}, il suffit d'appliquer
l'algorithme~\ref{algorithm_vecteur_propre}.



\begin{xremark}{expérience}

Soit $\left(  X_{i}\right)  _{1\leqslant i\leqslant N}$ avec $\forall i\in\left\{  1,...,N\right\} ,\,X_{i}\in\R^{p}$. Soit $\pa{P_1,\dots,P_p}$ l'ensemble des vecteurs propres normés de la matrice $XX'$ associés aux valeurs propres $\pa{\lambda_1,\dots,\lambda_p}$ classées par ordre décroissant de modules. On définit $\forall d \in \intervalle{1}{p}, \; W_d = \pa{P_1,\dots,P_d} \in M_{p,d}$. On définit alors l'inertie $I_d$ du nuage de points projeté sur l'espace vectoriel défini par $P_d$.
On suppose que le nuage de points est centré, alors~:

		$$
		\forall d \in \intervalle{1}{p}, \; I_d = \summy{k=1}{N} 
		\pa{P_d' X_k}^2 = tr \pa{X' P_d P_d' X} = tr \pa{XX' P_d P_d'} = \lambda_d
		$$

Comme $\pa{P_1,\dots,P_p}$ est une base orthonormée de $\R^p$, on en déduit que :

    $$
    I = \summy{k=1}{P} X_k'X_k = \summy{d=1}{N} I_d = \summy{d=1}{p} \lambda_d
    $$

De manière empirique, on observe fréquemment que la courbe $\pa{d,I_d}_{1 \infegal d \infegal p}$ montre un point
d'inflexion (figure~\ref{figure_point_inflexion}). Dans cet exemple, le point d'inflexion correspond à $d=4$. En
analyse des données, on considère empiriquement que seuls les quatres premières dimensions contiennent de l'information.

		\begin{figure}[ht]
    $$\frame{$\begin{array}[c]{c}\includegraphics[height=4cm, width=8cm] 
    {\filext{../dessin2/acp_inertie}}\end{array}$}$$
    \caption{Courbe d'inertie : point d'inflexion pour $d=4$, l'expérience montre que généralement, seules les
                projections sur un ou plusieurs des quatre premiers vecteurs propres reflètera l'information
                contenue par le nuage de points.}
    \label{figure_point_inflexion}
		\end{figure}

\end{xremark}




\newpage



\firstpassagedo{
	\begin{thebibliography}{99}
	\input{rn_bibliographie.tex}
	\end{thebibliography}
}


\input{../../common/livre_table_end.tex}
\input{../../common/livre_end.tex}










% insère une entrée dans la bibliographie
%		1 - identifiant
%		2 - année
%		3 - auteurs
%		4 - titre
%		5 - revue
%		6 - volume
%		7 - page début
%		8 - page fin


\bibitemstyle{Bottou1991} { 1991} { L. Bottou}
{Une approche théorique de l'apprentissage connexionniste, Application à la reconnaissance de la parole}
{Thèse de l'Université de Paris Sud, Centre d'Orsay}{}{0}{}

\bibitemstyle{Broyden1967} {1967} {C.G. Broyden}
{Quasi-Newton methods and their application to function minimization}
{Math. Comput}{21}{368}{}

\bibitemstyle{Cottrel1995} {1995} {M. Cottrel, B. Girard, M. Mangeas, C. Muller}
{Neural modeling for time series: a statistical stepwise methode for weight elimination}
{IEEE Transaction On Neural Networks}{6(6)}{0}{}



\bibitemstyle{Davidon1959} {1959} {C. W. Davidon}
{Variable metric method for minimization}
{A.E.C. Research and Development Report, ANL-5990}{}{0}{}

\bibitemstyle{Driancourt1996} {1996} {X.  Driancourt}
{Optimisation par descente de gradient stochastique de systèmes modulaires combinant réseaux de neurones et programmation dynamique, Application à la reconnaissance de la parole}
{Thèse de l'Université de Paris Sud, Centre d'Orsay}{}{0}{}

\bibitemstyle{Fletcher1963} {1963} { R. Fletcher, M. J. D. Powell}
{A rapidly convergent descent method for minimization},
{Computer Journal}{6}{163}{168}

\bibitemstyle{Fletcher1993} {1993} {R. Fletcher}
{An overview of Unconstrained Optimization}
{Numerical Analysis Report NA/149}{}{0}{}

\bibitemstyle{Kullback1951} {1951} { S. Kullback, R. A. Leibler }
{On information and sufficiency}
{Ann. Math. Stat.}{22}{79}{86}


\bibitemstyle{Moré1977} {1977} {J. J. Moré} 
{The Levenberg-Marquardt algorithm: Implementation and theory}
{Proceedings of the 1977 Dundee Conference on Numerical Analysis, G. A. Watson, ed., Lecture Notes in Mathematics, vol. 630, Springer-Verlag, Berlin}{}{105}{116}

\bibitemstyle{Rumelhart1986} {1986} { D. E. Rumelhart, G. E. Hinton, R. J. Williams}
{Learning internal representations by error propagation}
{Parallel distributed processing: explorations in the microstructures of cohniyionn MIT Press, Cambridge}{}{0}{}

\bibitemstyle{Saporta1990} {1990} { Gilbert Saporta  }
{Probabilités, analyse des données et statistique}
{Editions Technip}{}{0}{}

\bibitemstyle{Song1997} {1997} {Song Wang, Shaowei Xia}
{Self-organizing algorithm of robust PCA based on single layer NN}
{Proceedings of the 4th International Conference Document Analysis and Recognition (ICDAR)}{}{0}{}

